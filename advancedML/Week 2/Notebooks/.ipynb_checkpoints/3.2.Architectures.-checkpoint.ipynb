{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 3, Notebook 2: CNN Architectures\n",
    "\n",
    "Tutorial by Cher Bass\n",
    "(edited by Emma Robinson)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by importing the modules and Data that we need for the notebook. We start by testing on the MNIST dataset as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F #contains some useful functions like activation functions & convolution operations you can use\n",
    "\n",
    "import torchvision\n",
    "import numpy as np\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "device = torch.device(\"cuda: 0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# This is used to transform the images to Tensor and normalize it\n",
    "transform = transforms.Compose(\n",
    "   [transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5])])\n",
    "\n",
    "training = torchvision.datasets.MNIST(root='./data', train=True,\n",
    "                                       download=True, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(training, batch_size=8,\n",
    "                                         shuffle=True, num_workers=2)\n",
    "\n",
    "testing = torchvision.datasets.MNIST(root='./data', train=False,\n",
    "                                      download=True, transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(testing, batch_size=8,\n",
    "                                        shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('0', '1', '2', '3',\n",
    "          '4', '5', '6', '7', '8', '9')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet with pytorch\n",
    "\n",
    "ResNet was first introduced in 2016 as a way to deal with the gradient vanishing problem. This can occur when the network is too deep, and the gradients shrink to zero after a few back propagation steps. This can result in the parameter weights not being updated, since the gradient is zero.\n",
    "\n",
    "ResNets can counter this problem by allowing the gradients to flow directly backwards, by adding the additive resnet connections.\n",
    "\n",
    "An example of a resnet block (from the original 2016 paper) is illustrated below:\n",
    "\n",
    "![resnet-block](imgs/resnet-block.png)\n",
    "source: https://d2l.ai/chapter_convolutional-modern/resnet.html\n",
    "\n",
    "He, Kaiming, et al. \"Deep residual learning for image recognition.\" Proceedings of the IEEE conference on computer vision and pattern recognition. 2016.\n",
    "\n",
    "http://www.pabloruizruiz10.com/resources/CNNs/ResNet-PyTorch.html\n",
    "\n",
    "https://towardsdatascience.com/understanding-and-visualizing-resnets-442284831be8\n",
    "\n",
    "\n",
    "### Using existing ResNet \n",
    "\n",
    "It's possible to load existing networks using pytorch library torchvision - you can load these models using torchvision.models, which contains networks such as ResNet, Alexnet, VGG, Densenet, etc...\n",
    "https://pytorch.org/docs/stable/torchvision/models.html\n",
    "\n",
    "For example the following pretrained resnets models can be loaded in Pytorch:\n",
    "```python\n",
    "torchvision.models.resnet18(pretrained=True, **kwargs)\n",
    "```\n",
    "\n",
    "You can also load a model that hasn't been pretrained in the following way:\n",
    "```python\n",
    "torchvision.models.resnet18(pretrained=False, **kwargs)\n",
    "```\n",
    "\n",
    "You can find examples of how to use pretrained models in: https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html\n",
    "\n",
    "However, you will find that using a pretrained model doesn't always suit your needs. For example, the resnet models shown above have been trained on RGB images (i.e. they are 3 channels), which means that you can't use them without adjustment on grayscale images, or on 3D medical data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2.1 Programming your own ResNet\n",
    "\n",
    "The first thing we need to do is implement a `ResidualBlock` class, which will implement a single ResNet (2016) block, which includes the following steps: \n",
    "\n",
    "1. Convolution, followed by batchnorm, followed by relu\n",
    "2. Convolution, followed by batchnorm \n",
    "3. shortcut step, where \n",
    "    - the input is first transformed through a strided $1 \\times 1$ convolutional operation to match the dimensions of the output of the residual block\n",
    "    - added to the output of the convolutions. \n",
    "4. relu\n",
    "\n",
    "The only slightly challenging bit here is the first part of the shortcut step. So let's start by ignoring it to create the main body of the residual block. This will work provided we maintain input dimensions. \n",
    "\n",
    "#### To do 2.1.1 - Create the Residual  block\n",
    "\n",
    "As shown above the residual block performs `Conv2d > BatchNorm2d > ReLU > Conv2D BatchNorm2d > ADD > ReLU `. Let us create a `ResidualBlock` and define (parametrise) the required `Conv2d` and `BatchNorm2d` steps in the constructor (`__init__`):\n",
    "\n",
    "Tasks: edit (`__init__`) to input\n",
    "\n",
    "1. `self.conv1` a 2D convolution with arguments `in_channels=channels1,out_channels=channels2, kernel_size=3, stride=res_stride, padding=1, bias=False`. Here, `channels1, channels2 and res_stride` are input arguments to `__init()`. \n",
    "2. ` self.bn1` a 2D batchnorm layer with input `num_features=channels`\n",
    "2. `self.conv2` the second convolutional layer. *This time stride should be 1*. What should its input and output channel dimensions be? (note `kernel_size=3, stride=res_stride, padding=1, bias=False` as before)\n",
    "4. ` self.bn1` the second 2D batchnorm layer. What does it expect for the number of input features (`num_features`)\n",
    "\n",
    "See:\n",
    "https://pytorch.org/docs/stable/nn.html#torch.nn.Conv2d\n",
    "https://pytorch.org/docs/stable/nn.html#torch.nn.BatchNorm2d\n",
    "\n",
    "For PyTorch documentation for each of these functions, included argument variable names and expect input/output.\n",
    "\n",
    "Note, biases are set to `False` in the block as they are instead handled by the batchnorm layer see https://discuss.pytorch.org/t/why-does-the-resnet-model-given-by-pytorch-omit-biases-from-the-convolutional-layer/10990/2. Also, observe that the Relu layer is implemented in the forward pass function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, channels1,channels2,res_stride=1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.inplanes=channels1\n",
    "        # Exercise 2.1.1 construct the block without shortcut\n",
    "        self.conv1 = None\n",
    "        self.bn1 = None\n",
    "        self.conv2 = None\n",
    "        self.bn2 = None\n",
    "\n",
    "        if res_stride != 1 or channels2 != channels1:\n",
    "        # Exercise 2.1.3 the shortcut; create option for resizing input \n",
    "            self.shortcut=nn.Sequential()\n",
    "        else:\n",
    "            self.shortcut=nn.Sequential()\n",
    "            \n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # forward pass: Conv2d > BatchNorm2d > ReLU > \n",
    "        #Conv2D >  BatchNorm2d > ADD > ReLU\n",
    "        out=self.conv1(x)\n",
    "        out=self.bn1(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        # THIS IS WHERE WE ADD THE INPUT\n",
    "        #print('input shape',x.shape,self.inplanes)\n",
    "        out += self.shortcut(x)\n",
    "       # print('res block output shape',  out.shape)\n",
    "        # final ReLu\n",
    "        out = F.relu(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the steps of the forward pass. The networks performs the first convolution (followed by batchnorm and relu), then the second convolution (followed just by batchnorm). Then it adds the input. Here, the input is represented by the nn.Sequential() `self.shortcut` which we have currently left empty. Finally, the last operation of the block is a relu.\n",
    "\n",
    "#### To do 2.1.2 . Perform a test forward pass (keeping input and output dimensions constant\n",
    "\n",
    "1. Instantiate an instance of class ResidualBlock (create a network called `blk`)\n",
    "2. create a random tensor of size $5 \\times 3 \\times 100 \\times 100$ (which matches expected input dimensions $N,C_in,H,W$ the expected input dimensions of `nn.conv2d`\n",
    "3. Pass the input through a forward pass with number of input channels 3 and output channels 3\n",
    "\n",
    "**hint** look at how this was done in the last lecture. Remember - we don't need to explicitely call the forward function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  Student To do "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To do 2.1.3. Implement the shortcut \n",
    "\n",
    "Now, let us go back and edit the function to support resizing the input. This will allow us to downsample and change the number of feature dimensions within our residual block.\n",
    "\n",
    "Change line 14 in `ResidualBlock.__init__()` to implement a Sequential block with two steps:\n",
    "1. A $1 \\times 1 $ `nn.Conv2d` layer with `stride=res_stride,bias=False`.  This will support changes of spatial dimensions through strided convolutions and changes of feature dimensions through $1 \\times 1 $ convolutions. What should your input and output channels be to make it equivalent to the output of the residual block?\n",
    "2. batchnorm. Think carefully about the input dimension. \n",
    "\n",
    "Once you have done this, test the network again, but this time change the number of output\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  Student To do "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now have all the building blocks we need to build a residual network. In what follows we will construct a ResNet with four residual layers. Each layer will contain 2 residual blocks. \n",
    "\n",
    "#### To do 2.1.4 : Complete the Residual Network class\n",
    "\n",
    "**Step 1** Following the definition in the original paper the network starts with a  convolutional layer with a $7 \\times 7 $ kernel, followed by a batchnorm. However, as we intend to test on the MNIST (which is very small) lets change the $7 \\times 7 $ kernel to a $3 \\times 3 $ one (**check how this is implemented**) \n",
    "\n",
    "**Step 2 (Student complete)** Comment the function `_make_layer`. What is each line doing? Complete the class constructor, using `_make_layer` to create 4 residual layers,  with `num_blocks` residual blocks per layer, `num_strides[i]` strides per block (where $i$ indexes the layer, starting from the initial convolution) and `num_features[i]` represents the number of output channels per layer.\n",
    "\n",
    "**Step 4 (Student complete)** The last layer is a fully connected (softmax) layer. Complete this function. The number of inputs must match the number of outputs from the previous layer and the number of outputs must match the number of classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_strides, num_features, in_channels, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        # step 1. Initialising the network with a 3 x3 conv and batch norm\n",
    "        self.conv1 = nn.Conv2d(in_channels, num_features[0], kernel_size=3, stride=num_strides[0], padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        # Step 2: TO DO Using function make_layer() create 4 residual layers\n",
    "        # num_blocks per layer is given by input argument num_blocks (which is an array)\n",
    "        self.layer1 = None\n",
    "        self.layer2 = None\n",
    "        self.layer3 = None\n",
    "        self.layer4 = None\n",
    "        self.linear = None\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        layers = []\n",
    "        \n",
    "        for i in np.arange(num_blocks -1):\n",
    "            layers.append(block(self.in_planes, planes))\n",
    "            self.in_planes = planes \n",
    "        \n",
    "        layers.append(block(planes, planes, stride))\n",
    "        \n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        #print('init',out.shape)\n",
    "        out = self.layer1(out)\n",
    "        #print('layer1',out.shape)\n",
    "        out = self.layer2(out)\n",
    "       # print('layer2',out.shape)\n",
    "\n",
    "        out = self.layer3(out)\n",
    "        #print('layer3',out.shape)\n",
    "\n",
    "        out = self.layer4(out)\n",
    "       # print('layer4',out.shape)\n",
    "\n",
    "        \n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 5** Observe below, creation of an instance of class `ResNet`. This requires as argument the `ResidualBlock` class defined above. We hard code the argments for number of blocks, as well as lists defining the number of strides and features per layer. These lists have length 5 to encode also for the initial convolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_ResNet4(in_channels=1):\n",
    "    return ResNet(ResidualBlock,2, [1,1,2,2,2], [64,64,128,256,512], in_channels=in_channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To do 2.1.5:  run your ResNet on MNIST for classification\n",
    "Create a ResNet network and run with the same code as above for classification, and then test.\n",
    "Remember to define your loss function, optimizer (we suggest you first try SGD with lr=0.001 and momentum=0.9), dataloaders, and your resnet network. \n",
    "Then run the training and testing, as before.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------------------task 4 -----------------------------------------------------\n",
    "# Task 4: Train and test ResNet on MNIST dataset for classification\n",
    "# hints: define your resnet network, loss function, optimizer and dataloaders. \n",
    "# Then you can run the same training and testing code as above.\n",
    "# ----------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "resnet = None\n",
    "resnet = resnet.to(device)\n",
    "\n",
    "loss_fun = None\n",
    "loss_fun = loss_fun.to(device)\n",
    "\n",
    "optimizer = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1\n",
    "for epoch in range(epochs): \n",
    "\n",
    "    # enumerate can be used to output iteration index i, as well as the data \n",
    "    for i, (data, labels) in enumerate(train_loader, 0):\n",
    "        # Student to complete\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # print statistics (expecting loss to be output to variable `loss`)\n",
    "        ce_loss = loss.item()\n",
    "        if i % 10 == 0:\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                 (epoch + 1, i + 1, ce_loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#make an iterator from test_loader\n",
    "#Get a batch of testing images\n",
    "test_iterator = iter(test_loader)\n",
    "images, labels = test_iterator.next()\n",
    "\n",
    "images = images.to(device)\n",
    "labels = labels.to(device)\n",
    "\n",
    "y_score = resnet(images)\n",
    "# get predicted class from the class probabilities\n",
    "_, y_pred = torch.max(y_score, 1)\n",
    "\n",
    "print('Predicted: ', ' '.join('%5s' % classes[y_pred[j]] for j in range(8)))\n",
    "rows = 2\n",
    "columns = 4\n",
    "# plot y_score - true label (t) vs predicted label (p)\n",
    "fig2 = plt.figure()\n",
    "for i in range(8):\n",
    "    fig2.add_subplot(rows, columns, i+1)\n",
    "    plt.title('t: ' + classes[labels[i].cpu()] + ' p: ' + classes[y_pred[i].cpu()])\n",
    "    img = images[i] / 2 + 0.5     # this is to unnormalize the image\n",
    "    img = torchvision.transforms.ToPILImage()(img.cpu())\n",
    "    plt.axis('off')\n",
    "    plt.imshow(img)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = labels.data.cpu().numpy()\n",
    "y_pred = y_pred.data.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score\n",
    "\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred, average='macro')\n",
    "precision = precision_score(y_true, y_pred, average='macro')\n",
    "recall = recall_score(y_true, y_pred, average='macro')\n",
    "print('accuracy:', accuracy, ', f1 score:', f1, ', precision:', precision, ', recall:', recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional Exercise) Use ResNet for classification - CIFAR10\n",
    "\n",
    "Use the torch inbuilt ResNet for RBG images and train for classification on the CIFAR10 dataset.\n",
    "\n",
    "Here are some example images from the CIFAR10 datasets- we have 10 classes:\n",
    "\n",
    "![cifar10](imgs/cifar10.jpg)\n",
    "source: https://appliedmachinelearning.blog/2018/03/24/achieving-90-accuracy-in-object-recognition-task-on-cifar-10-dataset-with-keras-convolutional-neural-networks/\n",
    "\n",
    "You can load the CIFAR10 dataset using torchvision in the following way:\n",
    "```python\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=8,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(testset, batch_size=8,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "```\n",
    "You can use this tutorial as a reference for training on CIFAR10 - https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n",
    "\n",
    "Remember to define your loss function, optimizer, dataloaders, and your resnet network. \n",
    "Then run the training and testing, same as with MNIST.\n",
    "\n",
    "First, import the PyTorch ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "\n",
    "resnet_cifar = models.resnet18(pretrained=True)\n",
    "resnet_cifar = resnet_cifar.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------------------task 5 -----------------------------------------------------\n",
    "# Task 5: Train and test ResNet on CIFAR10 dataset for classification\n",
    "# hints: define your resnet network, loss function, optimizer and dataloaders. \n",
    "# Then you can run the same training and testing code as above.\n",
    "# ----------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=8,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = None\n",
    "test_loader = None\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "loss_fun = None\n",
    "loss_fun = loss_fun.to(device)\n",
    "\n",
    "optimizer = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "for epoch in range(epochs): \n",
    "\n",
    "    # enumerate can be used to output iteration index i, as well as the data \n",
    "    for i, (data, labels) in enumerate(train_loader, 0):\n",
    "        # Student to complete\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # print statistics\n",
    "        ce_loss = loss.item()\n",
    "        if i % 10 == 0:\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                 (epoch + 1, i + 1, ce_loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make an iterator from test_loader\n",
    "#Get a batch of testing images\n",
    "test_iterator = iter(test_loader)\n",
    "images, labels = test_iterator.next()\n",
    "images = images.to(device)\n",
    "labels = labels.to(device)\n",
    "\n",
    "y_score = resnet_cifar(images)\n",
    "# get predicted class from the class probabilities\n",
    "_, y_pred = torch.max(y_score, 1)\n",
    "\n",
    "print('Predicted: ', ' '.join('%5s' % classes[y_pred[j]] for j in range(8)))\n",
    "\n",
    "# plot y_score - true label (t) vs predicted label (p)\n",
    "fig2 = plt.figure()\n",
    "for i in range(8):\n",
    "    fig2.add_subplot(rows, columns, i+1)\n",
    "    plt.title('t: ' + classes[labels[i].cpu()] + ' p: ' + classes[y_pred[i].cpu()])\n",
    "    img = images[i] / 2 + 0.5     # this is to unnormalize the image\n",
    "    img = torchvision.transforms.ToPILImage()(img.cpu())\n",
    "    plt.axis('off')\n",
    "    plt.imshow(img)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = labels.data.cpu().numpy()\n",
    "y_pred = y_pred.data.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score\n",
    "\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred, average='macro')\n",
    "precision = precision_score(y_true, y_pred, average='macro')\n",
    "recall = recall_score(y_true, y_pred, average='macro')\n",
    "print('accuracy:', accuracy, ', f1 score:', f1, ', precision:', precision, ', recall:', recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.2.2 Image segmentation with pytorch using U-net\n",
    "\n",
    "U-net was first developed in 2015 by Ronneberger et al., as a segmentation network for biomedical image analysis.\n",
    "It has been extremely successful, with 9,000+ citations, and many new methods that have used the U-net architecture since.\n",
    "\n",
    "\n",
    "The architecture of U-net is based on the idea of using skip connections (i.e. concatenating) at different levels of the network to retain high, and low level features.\n",
    "\n",
    "Here is the architecture of a U-net:\n",
    "\n",
    "---\n",
    "\n",
    "![U-net](imgs/unet.png)\n",
    "Ronneberger, Olaf, Philipp Fischer, and Thomas Brox. \"U-net: Convolutional networks for biomedical image segmentation.\" International Conference on Medical image computing and computer-assisted intervention. Springer, Cham, 2015."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two-photon microscopy dataset of cortical axons\n",
    "\n",
    "In this tutorial we use a dataset of cortical neurons with their corresponding segmentation binary labels.\n",
    "\n",
    "These images were collected using in-vivo two-photon microscopy from the mouse somatosensory cortex. To generate the 2D images, a max projection was used over the 3D stack. The labels are binary segmentation maps of the axons.\n",
    "\n",
    "Here we will use 100 [64x64] crops during training and validation. \n",
    "\n",
    "These are some example images [256x256] from the original dataset:\n",
    "![axon_dataset](imgs/axon_dataset.png)\n",
    "\n",
    "Bass, Cher, et al. \"Image synthesis with a convolutional capsule generative adversarial network.\" Medical Imaging with Deep Learning (2019).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load modules\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from AxonDataset import AxonDataset\n",
    "import torch.nn as nn\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import time\n",
    "import torch.nn.functional as F\n",
    "import torchvision.utils as vutils\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting parameters\n",
    "timestr = time.strftime(\"%d%m%Y-%H%M\")\n",
    "__location__ = os.path.realpath(\n",
    "    os.path.join(os.getcwd(), os.path.dirname('__file__')))\n",
    "\n",
    "print(__location__)\n",
    "\n",
    "path = os.path.join(__location__,'results')\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "    \n",
    "# Define your batch_size\n",
    "batch_size = 16\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a dataloader\n",
    "\n",
    "In this example, a custom dataloader was created, and we import it from `AxonDataset.py`\n",
    "\n",
    "We utilise the `torch.utils.data.sampler.SubsetRandomSampler` to create two DataLoaders for train and validation. Here, a random a subset of 20% of subject indices are selected for validation. The remaining 80% are used for training. The lists of train and validation subjects are passed to `torch.utils.data.sampler.SubsetRandomSampler` to create bespoke train/validation samplers; these are passed to the `DataLoader` using the argument `sampler,` and override the default use of `shuffle`.\n",
    "\n",
    "#### 3.2.1 Create a list of random indices for train and validation sets (**hint** use np.random.choice)\n",
    "\n",
    "Check you understand how the bespoke samplers are implemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First we create a dataloader for our example dataset- two photon microscopy with axons\n",
    "axon_dataset = AxonDataset(data_name='org64', type='train')\n",
    "\n",
    "# -----------------------------------------------------task 1----------------------------------------------------------------\n",
    "# Task 1: create a random list of indices for training and testing with a 80%,20% split\n",
    "\n",
    "# We need to further split our training dataset into training and validation sets.\n",
    "# Define the indices\n",
    "indices = None # start with all the indices in training set\n",
    "split = None # define the split size\n",
    "\n",
    "# Get indices for train and validation datasets, and split the data\n",
    "validation_idx = None\n",
    "train_idx = None\n",
    "# ----------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# feed indices into the sampler\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "validation_sampler = SubsetRandomSampler(validation_idx)\n",
    "\n",
    "# Create a dataloader instance \n",
    "train_loader = torch.utils.data.DataLoader(axon_dataset, batch_size = batch_size,\n",
    "                                           sampler=train_sampler) \n",
    "val_loader = torch.utils.data.DataLoader(axon_dataset, batch_size = batch_size,\n",
    "                                        sampler=validation_sampler) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a U-net \n",
    "\n",
    "We next build our u-net network.\n",
    "\n",
    "First we define a layer `double_conv` that performs 2 sets of convolution followed by ReLu.This is set up as a `nn.Sequential(` block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define U-net\n",
    "def double_conv(in_channels, out_channels, padding=1):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=padding),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=padding),\n",
    "        nn.ReLU(inplace=True)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to define how we perform an downsample and an upsample step. The original U-net performs downsampling through a $2 \\times 2 $ max pool (however, strided convolutions are equally viable). Upsampling is performed through use of `nn.Upsample` (https://pytorch.org/docs/stable/nn.html#torch.nn.Upsample), which interpolates the data to a higher resolution grid. The function expects arguments `scale_factor` and (interpolation) `mode`. There are several options for the interpolation mode; we recommend bilinear. In this example we upsample by a `scale_factor` of 2 each time (to match the $2\\times 2$ max pool used during downsampling). \n",
    "\n",
    "Thus, in what follows, a single level of encoding can be represented as:\n",
    "\n",
    "`conv1 = self.dconv_down1(x)\n",
    " conv1 = self.dropout(conv1)\n",
    " x = self.maxpool(conv1)`\n",
    "        \n",
    "In other words a double convolution followed by a maxpool. Here, a dropout layer is inserted between the convolutional layer and the maxpool for regularisation. An alternative approach is to insert a batchnorm between the `nn.Conv2d` and the `nn.ReLU` e.g. https://github.com/milesial/Pytorch-UNet\n",
    "\n",
    "A single level of decoding might be represented as:\n",
    "\n",
    "`deconv4 = self.upsample(conv5)\n",
    " deconv4  = self.dconv_up4(deconv4)\n",
    " deconv4 = self.dropout(deconv4)`\n",
    " \n",
    "However, we are missing something vital...\n",
    "\n",
    "### Skip connections\n",
    "\n",
    "The U-net is a symmetric network with equal numbers of encoding and decoding layers. These form pairs where the spatial dimensions of each encoder/decoder layer in the pair are consistent.\n",
    "\n",
    "A key feature of the U-net is that to support segmentation of sharp boundaries with preservation of high spatial resolution features it is necessary to pass features learnt during encoding across the network. The theory is that the early layers, with their small-receptive fields, learn the high-spatial frequency information (i.e. they act as edge detectors and/or texture filters). As the receptive field increases during encoding spatial specicity is lost, but spatial localisation (where class relevant objects broadly are in the image) is gained. In order to import the high spatial frequency information of the early encoding layers into the final decoding layers the *activations* learnt during encoding are directly concatenated onto the upsampled activations of the paired decoding layer.\n",
    "\n",
    "In other words for the first decoding layer (which for a 5-layer U-Net is the layer that directly follows the bottleneck `conv5`) is:\n",
    "\n",
    "`deconv4 = self.upsample(conv5)\n",
    " deconv4 = torch.cat([deconv4, conv4], dim=1)\n",
    " deconv4  = self.dconv_up4(deconv4)\n",
    " deconv4 = self.dropout(deconv4)`\n",
    " \n",
    " The activations (output) of convolution layer conv (`conv4`) is directly concatenated to the output of `self.upsample` where concatenation is performed on the channel axis (`axis=1`); Thus putting this all together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2.2. Building a U-Net\n",
    "\n",
    "We then define our U-net network.\n",
    "\n",
    "We first initialise all the different layers in the network in `__init__`:\n",
    "1. `self.dconv_down1` is a double convolutional layer (defined above)\n",
    "2. `self.maxpool` is a max pooling layer that is used to reduce the size of the input, and increase the receptive field\n",
    "3. `self.upsample` is an upsampling layer that is used to increase the size of the input\n",
    "4. `dropout` is a dropout layer that is applied to regularise the training\n",
    "5. `dconv_up4` is also a double convolutional layer- note that it takes in additional channels from previous layers (i.e. the skip connections).\n",
    "\n",
    "\n",
    "### To do 2.2.1  complete the forward pass\n",
    "\n",
    "1. Following the example for conv1 complete encoder layers 2,3 and 4. How many features does each layer have?\n",
    "2. Complete layer `conv5`; this is the bottleneck layer (the bottom of the network) and thus has no maxpool.\n",
    "2. Using the upsampling and skip connection example above implement the decoder layers `deconv4`,`deconv3`,`deconv2`,`deconv1`. Note - you should be concatenating the activations of the paired layers from the encoding path; these are the activations with the matching spatial dimensions (see above notes for more details)\n",
    "5. We are expecting class labels as output; thus the output requires a sigmoid transformation; check you understand what this does?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class UNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.dconv_down1 = double_conv(1, 32)\n",
    "        self.dconv_down2 = double_conv(32, 64)\n",
    "        self.dconv_down3 = double_conv(64, 128)\n",
    "        self.dconv_down4 = double_conv(128, 256)\n",
    "        self.dconv_down5 = double_conv(256, 512)\n",
    "\n",
    "        self.maxpool = nn.MaxPool2d(2)\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "        self.dropout = nn.Dropout2d(0.5)\n",
    "        self.dconv_up4 = double_conv(256 + 512, 256)\n",
    "        self.dconv_up3 = double_conv(128 + 256, 128)\n",
    "        self.dconv_up2 = double_conv(128 + 64, 64)\n",
    "        self.dconv_up1 = double_conv(64 + 32, 32)\n",
    "\n",
    "        self.conv_last = nn.Conv2d(32, 1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        #######   ENCODER ###############\n",
    "        \n",
    "        conv1 = self.dconv_down1(x)\n",
    "        conv1 = self.dropout(conv1)\n",
    "        x = self.maxpool(conv1)\n",
    "\n",
    "        # --------------------------------------------------- task 2.2.1 ----------------------------------------------------------\n",
    "        # implement encoder layers conv2, conv3 and conv4\n",
    "        \n",
    "        \n",
    "\n",
    "        # --------------------------------------------------- task 2.2.2 ----------------------------------------------------------\n",
    "        # implement bottleneck layer conv5\n",
    "        \n",
    "        conv5 = self.dconv_down5(x)\n",
    "        conv5 = self.dropout(conv5)\n",
    "        # ---------------------------------------------------------------------------------------------------------------------\n",
    "       \n",
    "        #######   DECODER ###############\n",
    "        \n",
    "        # --------------------------------------------------- task 2.2.3 ----------------------------------------------------------\n",
    "        # Implement the decoding layers\n",
    "        \n",
    "\n",
    "        #---------------------------------------------------------------------------------------------------------------------\n",
    "        out = F.sigmoid(self.conv_last(deconv1))\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To save time we initialise the network with a previously trained network by loading the weights\n",
    "\n",
    "*for practical reasons training this network from scratch will take too long, and require large computational resources*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise network - and load weights\n",
    "net = UNet()\n",
    "#net.load_state_dict(torch.load(path+'/'+'model.pt')) #this function loads a pretrained network\n",
    "net.load_state_dict(torch.load(path+'/'+'model.pt',map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining an appropriate loss function\n",
    "We next define our loss function - in this case we use Dice loss, a commonly used loss for image segmentation.\n",
    "\n",
    "The Dice coefficient can be used as a loss function, and is essentially a measure of overlap between two samples.\n",
    "\n",
    "Dice is in the range of 0 to 1, where a Dice coefficient of 1 denotes perfect and complete overlap. The Dice coefficient was originally developed for binary data, and can be calculated as:\n",
    "\n",
    "$Dice = \\dfrac{2|A\\cap B|}{|A| + |B|}$\n",
    "\n",
    "where $|A\\cap B|$ represents the common elements between sets $A$ and $B$, and $|A|$ represents the number of elements in set $A$ (and likewise for set $B$).\n",
    "\n",
    "For the case of evaluating a Dice coefficient on predicted segmentation masks, we can approximate  $|A\\cap B|$ as the element-wise multiplication between the prediction and target mask, and then sum the resulting matrix.\n",
    "\n",
    "An **alternative loss** function would be pixel-wise cross entropy loss. It would examine each pixel individually, comparing the class predictions (depth-wise pixel vector) to our one-hot encoded target vector.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dice loss\n",
    "def dice_coeff(pred, target):\n",
    "    \"\"\"This definition generalize to real valued pred and target vector.\n",
    "    This should be differentiable.\n",
    "    pred: tensor with first dimension as batch\n",
    "    target: tensor with first dimension as batch\n",
    "    \"\"\"\n",
    "\n",
    "    smooth = 1.\n",
    "    epsilon = 10e-8\n",
    "\n",
    "    # have to use contiguous since they may from a torch.view op\n",
    "    iflat = pred.contiguous().view(-1)\n",
    "    tflat = target.contiguous().view(-1)\n",
    "    intersection = (iflat * tflat).sum()\n",
    "\n",
    "    A_sum = torch.sum(iflat * iflat)\n",
    "    B_sum = torch.sum(tflat * tflat)\n",
    "\n",
    "    dice = (2. * intersection + smooth) / (A_sum + B_sum + smooth)\n",
    "    dice = dice.mean(dim=0)\n",
    "    dice = torch.clamp(dice, 0, 1.0-epsilon)\n",
    "\n",
    "    return  dice\n",
    "\n",
    "# cross entropy loss\n",
    "loss_BCE = nn.BCEWithLogitsLoss()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the penalty term `smooth` is added to prevent division by zero.\n",
    "\n",
    "As before, we define the optimiser to train our network - here we use Adam.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define your optimiser\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, net.parameters()), lr=1e-05, betas=(0.5, 0.999))\n",
    "optimizer.zero_grad()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and evaluating our segmentation network\n",
    "We next train and evaluate our network \n",
    "\n",
    "note that the results are saved to a folder \\results - so please check that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=10\n",
    "save_every=10\n",
    "all_error = np.zeros(0)\n",
    "all_error_L1 = np.zeros(0)\n",
    "all_error_dice = np.zeros(0)\n",
    "all_dice = np.zeros(0)\n",
    "all_val_dice = np.zeros(1)\n",
    "all_val_error = np.zeros(0)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    ##########\n",
    "    # Train\n",
    "    ##########\n",
    "    t0 = time.time()\n",
    "    for i, (data, label) in enumerate(train_loader):\n",
    "        \n",
    "        # setting your network to train will ensure that parameters will be updated during training, \n",
    "        # and that dropout will be used\n",
    "        net.train()\n",
    "        net.zero_grad()\n",
    "\n",
    "        target_real = torch.ones(data.size()[0])\n",
    "        batch_size = data.size()[0]\n",
    "        pred = net(data)\n",
    "        \n",
    "        # dice loss = 1-dice_coeff\n",
    "        # ----------------------------------------------- task 3 ------------------------------------------------------------\n",
    "        # Task 3: change loss function here\n",
    "        err = 1- dice_coeff(pred, label)\n",
    "        err = loss_BCE(pred, label)\n",
    "        # -------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "        dice_value = dice_coeff(pred, label).item()\n",
    "\n",
    "        err.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        time_elapsed = time.time() - t0\n",
    "        print('[{:d}/{:d}][{:d}/{:d}] Elapsed_time: {:.0f}m{:.0f}s Loss: {:.4f} Dice: {:.4f}'\n",
    "              .format(epoch, epochs, i, len(train_loader), time_elapsed // 60, time_elapsed % 60,\n",
    "                      err.item(), dice_value))\n",
    "\n",
    "        if i % save_every == 0:\n",
    "            # setting your network to eval mode to remove dropout during testing\n",
    "            net.eval()\n",
    "\n",
    "            vutils.save_image(data.data, '%s/epoch_%03d_i_%03d_train_data.png' % (path, epoch, i),\n",
    "                                  normalize=True)\n",
    "            vutils.save_image(label.data, '%s/epoch_%03d_i_%03d_train_label.png' % (path, epoch, i),\n",
    "                                  normalize=True)\n",
    "            vutils.save_image(pred.data, '%s/epoch_%03d_i_%03d_train_pred.png' % (path, epoch, i),\n",
    "                                  normalize=True)\n",
    "\n",
    "            error = err.item()\n",
    "\n",
    "            all_error = np.append(all_error, error)\n",
    "            all_dice = np.append(all_dice, dice_value)\n",
    "\n",
    "    # #############\n",
    "    # # Validation\n",
    "    # #############\n",
    "    mean_error = np.zeros(0)\n",
    "    mean_dice = np.zeros(0)\n",
    "    t0 = time.time()\n",
    "    for i, (data, label) in enumerate(val_loader):\n",
    "\n",
    "        net.eval()\n",
    "        batch_size = data.size()[0]\n",
    "\n",
    "        data, label = Variable(data), Variable(label)\n",
    "        pred = net(data)\n",
    "        \n",
    "        # ----------------------------------------------- task 3 ------------------------------------------------------------\n",
    "        # Task 3: change loss function here\n",
    "        err = 1-dice_coeff(pred, label)\n",
    "        # err = loss_BCE(pred, label)\n",
    "        # -------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "        # compare generated image to data-  metric\n",
    "        dice_value = dice_coeff(pred, label).item()\n",
    "\n",
    "        if i == 0:\n",
    "            vutils.save_image(data.data, '%s/epoch_%03d_i_%03d_val_data.png' % (path, epoch, i),\n",
    "                              normalize=True)\n",
    "            vutils.save_image(label.data, '%s/epoch_%03d_i_%03d_val_label.png' % (path, epoch, i),\n",
    "                              normalize=True)\n",
    "            vutils.save_image(pred.data, '%s/epoch_%03d_i_%03d_val_pred.png' % (path, epoch, i),\n",
    "                              normalize=True)\n",
    "\n",
    "        error = err.item()\n",
    "        mean_error = np.append(mean_error, error)\n",
    "        mean_dice = np.append(mean_dice, dice_value)\n",
    "\n",
    "    all_val_error = np.append(all_val_error, np.mean(mean_error))\n",
    "    all_val_dice = np.append(all_val_dice, np.mean(mean_dice))\n",
    "\n",
    "    time_elapsed = time.time() - t0\n",
    "\n",
    "    print('Elapsed_time: {:.0f}m{:.0f}s Val dice: {:.4f}'\n",
    "          .format(time_elapsed // 60, time_elapsed % 60, mean_dice.mean()))\n",
    "    \n",
    "    \n",
    "    num_it_per_epoch_train = ((train_loader.dataset.x_data.shape[0] * (1 - 0.2)) // (\n",
    "            save_every * batch_size)) + 1\n",
    "    epochs_train = np.arange(1,all_error.size+1) / num_it_per_epoch_train\n",
    "    epochs_val = np.arange(0,all_val_dice.size)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(epochs_val, all_val_dice, label='dice_val')\n",
    "    plt.xlabel('epochs')\n",
    "    plt.legend()\n",
    "    plt.title('Dice score')\n",
    "    plt.savefig(path + '/dice_val.png')\n",
    "    plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results \n",
    "the results are saved to a folder \\results - so please check that:\n",
    "\n",
    "The results are saved per epoch for both training and validation, and are saved as the \n",
    "1. real data, \n",
    "2. binary labels, \n",
    "3. predicted labels. \n",
    "\n",
    "In this example since we trained on a small sample of the data (100 crops) the results are far from optimal, and are likely to overfit to the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3\n",
    "\n",
    "1. Change the dice loss to a cross entropy loss in the code - is dice loss or cross entropy loss better?\n",
    "2. run the training with dropout - what's the effect?\n",
    "\n",
    "**Note down your dice validation scores for each experiment, then change**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
