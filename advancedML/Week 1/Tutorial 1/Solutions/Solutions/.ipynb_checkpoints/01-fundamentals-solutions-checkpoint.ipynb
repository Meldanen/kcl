{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 1: fundamentals\n",
    "\n",
    "Noteboook by Mark Graham, adapted by Emma Robinson\n",
    "\n",
    "## Exercise 2: Implementing a single neuron classifier through logistic regression\n",
    "\n",
    "In this section we will train a classifier to predict if a neonate is preterm based on volume measures of 86 brain volumes. We will code up and train a logistic regression classifier from scratch.\n",
    "\n",
    "### Import the data\n",
    "The data are in the file \"prem_vs_termwrois.pkl\". The final column indicates whether each data set was collected from a term or preterm baby (scanned at term equivalent age). The data represent mean vales of three different types of cortical imaging data: cortical thickness, cortical folding and cortical myelination, all averaged within 100 regions of interest ROIS on the surface. This gives 300 features in total. \n",
    "\n",
    "<img src=\"imgs/cortical_rois.png\" alt=\"Drawing\" style=\"width: 800px;\"/>\n",
    "\n",
    "There are 101 babies, 50 terms and 51 preterms. The code below loads the file and splits the data randomly into a train and test set. The data is transposed such that the rows reflect features and the columns examples (as expected from the lectures notation). A row of ones is added to each dataset to allow modelling of the bias term.\n",
    "\n",
    "Run the code, be sure to understand what each line is doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Read the data\n",
    "df = pd.read_pickle(\"prem_vs_termwrois.pkl\")\n",
    "data = df.values[:,:-2]\n",
    "y = df.values[:,-1]\n",
    "\n",
    "# create a test and train split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, y, test_size=0.1, random_state=42)\n",
    "\n",
    "# Create feature matrix\n",
    "X_train = X_train.T\n",
    "X_test=X_test.T\n",
    "bias_row=np.ones((1,X_train.shape[1]))\n",
    "print(X_train.shape,X_test.shape)\n",
    "\n",
    "print(bias_row.shape)\n",
    "X_train = np.concatenate((np.ones((1,X_train.shape[1])),X_train))\n",
    "X_test = np.concatenate((np.ones((1,X_test.shape[1])),X_test))\n",
    "\n",
    "\n",
    "\n",
    "# set variables for numbers of feature and examples to improve readabiity of code\n",
    "n_features=X_train.shape[0]-1\n",
    "n_examples=X_train.shape[1]\n",
    "\n",
    "print('Dimension of X is ', X_train.shape,data.shape)\n",
    "print('Dimension of y is ', y.shape)\n",
    "\n",
    "print('Number of features', n_features)\n",
    "print('Number of examples', n_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eyeball the data\n",
    "The following code plots histograms of a single feature (brain volume) for preterms vs terms. Run the code for a few different values of 'feature'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#### STUDENT'S CODE HERE ####\n",
    "# try values between 0 and 300\n",
    "feature = 150\n",
    "\n",
    "plt.hist(X_train[feature+1, y_train==0], bins=30)\n",
    "plt.hist(X_train[feature+1, y_train==1], bins=30)\n",
    "plt.xlabel('Feature number {}/86'.format(feature))\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend(['Term','Preterm']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "\n",
    "Our predictions for a single input can be written:\n",
    "$$ f= f(z) = \\dfrac{1}{1+e^{-z}} $$\n",
    "\n",
    "Where, for logistic regression, $f$ is the sigmoid function and:\n",
    "\n",
    "$$z=w_0 + w_1x_1 + w_2x_2 +w_3x_3....+w_m x_m$$\n",
    "\n",
    "Here $w_0$ is the bias term, $w_1,w_2....w_m$ are the weights;, $m$ is the number of features and $\\mathbf{x}$ is a single example from our training set $X \\in \\mathbb{R}^{m\\times n}$ (of size $ m\\times n$).\n",
    "\n",
    "\n",
    "### Implementation of the forward pass\n",
    "\n",
    "We could calculate $f$ in one line of code, but it will come in handy when considering backpropagation later to consider the computation in stages, with each stage consisting of a simple module:\n",
    "\n",
    "$$\n",
    "\\begin{align} \n",
    "\\mathbf{Z} &= \\mathbf{W}^T \\mathbf{X} \\\\\n",
    "\\mathbf{F}=f(\\mathbf{Z}) &= \\dfrac{1}{1+e^{-\\mathbf{Z}}} \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Implemented using vectorisation. \n",
    "\n",
    "### Task 2.1 Estimate $\\mathbf{Z}$: \n",
    "\n",
    "Write a function $z(w,x)$ that uses vectorisation to linearly transform data matrix $\\mathbf{X}$ using the weights matrix $\\mathbf{W}$.\n",
    "\n",
    "**Hint** $\\mathbf{X}$ has size $m_{features} \\times n_{examples}$. What size should the output matrix $\\mathbf{Z}$ be?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def z(w,x):\n",
    "    #### STUDENT'S CODE HERE ####\n",
    "    return np.matmul(w.T, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an matrix of zeros to initialise $\\mathbf{W}$. Verify your function $z(w,x)$ gives an output of the expected dimension.\n",
    "\n",
    "- What dimension should $\\mathbf{W}$ be? \n",
    "- What dimension should the product $\\mathbf{W}^T\\mathbf{X}$ be? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### STUDENT'S CODE HERE ####\n",
    "# Answer:\n",
    "w = np.zeros((X_train.shape[0],1))\n",
    "output = z(w,X_train)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.2 Implement Sigmoid function f: \n",
    "\n",
    "Now write a function to compute $f(\\mathbf{Z})$, our logistic regression function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(z):\n",
    "    return 1 / (1+ np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify your softmax looks right by running this plotting code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = np.linspace(-10,10)\n",
    "outputs = f(inputs)\n",
    "plt.plot(inputs, outputs)\n",
    "plt.xlabel('z')\n",
    "plt.ylabel('sigmoid(z)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now in a position to compute some predictions $\\mathbf{\\hat{y}}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = f(z(w,X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are these predictions any good? Let's take a look at the accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y, y_pred, threshold = 0.5):\n",
    "    y_pred_thresholded = y_pred > threshold\n",
    "    correct_predictions = np.sum(y==y_pred_thresholded)  \n",
    "    total_predictions = np.shape(y)\n",
    "    accuracy = 100 * correct_predictions / total_predictions\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = f(z(w, X_train))\n",
    "print(accuracy(y_train, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the predictions ```y_pred```, what does this initial prediction return and why? Enter your answer in the box below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get them to understand that this returning 0.5 because weights are all zero!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.3 Implement Cross Entropy Loss:\n",
    "\n",
    "Accuracy is easy to intepret, but can't be optimised using gradient descent. We need a measure of our prediction quality that can be. A typical loss function used in  classification problems is cross-entropy:\n",
    "\n",
    "$$L(y_i,f(z_i)) = - y_i \\ln(f(z_i)) - (1-y_i) \\ln(1-f(z_i))$$\n",
    "\n",
    "This may be implemented using vectorisation as:\n",
    "\n",
    "$$L(\\mathbf{Y},\\mathbf{F}) = - \\mathbf{Y} \\ln(\\mathbf{F}) - (1-\\mathbf{Y}) \\ln(1-\\mathbf{F})$$\n",
    "\n",
    "This returns a vector of losses $(L_1,L_2....L_n)$ estimated for all training examples n. We require the total cost estimated as:\n",
    "\n",
    "$$ J(\\mathbf{W})= \\frac{1}{n} \\sum_i L_i(y_i,f(z_i)) $$\n",
    "\n",
    "Implement the Cross-Entropy loss and return the total cost (**hint** using numpy functions for vectorisation). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(y, y_pred):\n",
    "    epsilon = 1e-5\n",
    "    # note the negative sign so that the loss decreases as our predictions get better\n",
    "    # we must add a small penaty term to prevent calculation of log(0)\n",
    "    L = - y * np.log(y_pred+epsilon) - (1-y) * np.log(1-y_pred+epsilon) \n",
    "    J = np.mean(L)\n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_loss= loss(y_train,y_pred)\n",
    "print(total_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.4 Implement Forward Pass\n",
    "\n",
    "\n",
    "We now have all the components of the forward pass for our logistic regression. Write a full forward pass that takes data, targets and a weight matrix and performs the forward pass to calculate the loss:\n",
    "\n",
    "**hint - One line** You have already defined all the vectorised functions you need above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(X, y, w):\n",
    "    F = f(z(w,X))\n",
    "    print('Loss: {}'.format(loss(y,y_pred)))\n",
    "    print('Accuracy: {}'.format(accuracy(y,y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#perform forward pass\n",
    "forward_pass(X_train,y_train, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.5 Implement backwards pass\n",
    "\n",
    "We're now ready to try and adjust our parameters $\\mathbf{W}$ in order to optimise our predictions. To do this we need to calculate the change in our loss function with respect to our parameters, $\\dfrac{\\partial L}{\\partial \\mathbf{W}}$. \n",
    "\n",
    "Recalling our staged calculation of the logistic regression (in vectorised form):\n",
    "\n",
    "$$\n",
    "\\mathbf{Z} = \\mathbf{W}^T \\mathbf{X} \\\\\n",
    "\\mathbf{F}= \\dfrac{1}{1+e^{- \\mathbf{Z}}} \\\\\n",
    "\\mathbf{L}  =  - \\mathbf{Y} \\ln(\\mathbf{F}) - (1-\\mathbf{Y}) \\ln(1-\\mathbf{F})\n",
    "$$\n",
    "\n",
    "We can write the vectorised gradients for each individual stage (see lecture slides): \n",
    "\n",
    "$$\n",
    "\\dfrac{\\partial L}{\\partial f} = \\dfrac{\\mathbf{F} - \\mathbf{Y}}{\\mathbf{F}(1-\\mathbf{F})}\\\\\n",
    "\\dfrac{\\partial f}{\\partial z} = \\mathbf{F}(1-\\mathbf{F}) \\\\\n",
    "\\dfrac{\\partial z}{\\partial w} = \\mathbf{X}^T\n",
    "$$\n",
    "\n",
    "And compose through the chain rule:\n",
    "\n",
    "$$ \n",
    "\\dfrac{\\partial L}{\\partial w} = \\dfrac{\\partial L}{\\partial f} \\cdot \\dfrac{\\partial f}{\\partial z} \\cdot\\dfrac{\\partial z}{\\partial w} \\\\\n",
    "\\dfrac{\\partial L}{\\partial w} = \\dfrac{\\mathbf{F} - \\mathbf{Y}}{\\mathbf{F}(1-\\mathbf{F})} \\cdot \\mathbf{F}(1-\\mathbf{F}) \\cdot \\mathbf{X}^T\n",
    "$$\n",
    "\n",
    "Which can be simplified by cancelling $ \\mathbf{F}(1-\\mathbf{F})$ terms in both the numerator and the denominator: \n",
    "\n",
    "$$ \\dfrac{\\partial L}{\\partial w} = (\\mathbf{F} - \\mathbf{Y}) \\mathbf{X}^T $$\n",
    "\n",
    "Let's calculate the gradient of our loss, $\\dfrac{\\partial L}{\\partial \\mathbf{W}}$, for a **single** input, $\\mathbf{x}$. Fill in the calculations of the backward pass in the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.zeros(X_train.shape[0])\n",
    "\n",
    "# select just the first example here\n",
    "x = X_train[:,0]\n",
    "y_single = y[0]\n",
    "print('The true value of y is: {}'.format(y_single))\n",
    "\n",
    "# calculate the forward pass, and store the outputs at each stage\n",
    "Z = z(w,x)\n",
    "F = f(Z)\n",
    "print('Our prediction for y is: {}'.format(F))\n",
    "\n",
    "l = loss(y_single,F)\n",
    "print('The loss is {}'.format(l))\n",
    "\n",
    "# now enter the backwards pass here, \n",
    "#implementing using the equations above:\n",
    "dl_dw = (F-y_single)*x.T\n",
    "\n",
    "print(dl_dw.shape,w.shape,x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check this gradient calculation is correct by updating our weights vector and looking at our new prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The gradient is in the direction of increasing loss,\n",
    "# so we subtract the gradient from w.\n",
    "w = w - 0.001 * dl_dw\n",
    "Z = z(w,x)\n",
    "F = f(Z)\n",
    "l = loss(y_single,F)\n",
    "print('Our updated prediction for y is: {}'.format(F))\n",
    "print('The loss is {}'.format(l))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good! We have only updated $\\mathbf{w}$ using information from a single data point. In practice we want to use all the data available. \n",
    "\n",
    "**To do** \n",
    "1. Perform the backward pass again, but this time with vectorisation. Check that the shape of `dL_dw` is what you would expect.\n",
    "2. Estimate the full cost over all training examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.zeros(X_train.shape[0])\n",
    "\n",
    "# calculate the forward pass, and store the outputs at each stage\n",
    "Z = z(w,X_train)\n",
    "F = f(Z)\n",
    "l = loss(y_train, F)\n",
    "\n",
    "# To do - implement the backward pass\n",
    "dL_dw = np.matmul((F-y_train),X_train.T) \n",
    "\n",
    "print('dL_dw has shape: {}'.format(dL_dw.shape))\n",
    "\n",
    "grad_mean = dL_dw/n_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Putting it all together: the training loop\n",
    "\n",
    "We now have everything we need to train a logistic regression classifier using backprop. Fill out the training loop below using the code you have already written in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise w to all zeros\n",
    "w = np.zeros(X_train.shape[0])\n",
    "epsilon=1e-5\n",
    "# centre X\n",
    "X_centred = np.ones_like(X_train)\n",
    "X_centred[1:] = (X_train[1:] -X_train[1:].mean(axis=1,keepdims=True)) / (X_train[1:].std(axis=1,keepdims=True)+epsilon)\n",
    "\n",
    "# we'll store the loss and accuracy in these lists during training\n",
    "loss_record = []\n",
    "accuracy_record = []\n",
    "\n",
    "num_iterations = 2000\n",
    "learning_rate = 1e-3\n",
    "\n",
    "for i in range(num_iterations):\n",
    "    # forward pass - get predictions\n",
    "    #### STUDENT CODE HERE####\n",
    "    # answer\n",
    "    Z = z(w,X_centred)\n",
    "    F = f(Z)\n",
    "    l = loss(y_train,F)\n",
    "    \n",
    "    # store the loss/ accuracy at this iteration\n",
    "    loss_record.append(loss(y_train,F))\n",
    "    accuracy_record.append(accuracy(y_train,F))\n",
    "    \n",
    "    #backwards pass to get gradients\n",
    "    #### STUDENT CODE HERE #### \n",
    "    # answer:\n",
    "    dL_dw = np.matmul((F-y_train),X_centred.T) \n",
    "\n",
    "    grad_mean = dL_dw/n_examples\n",
    "    \n",
    "    # update the \n",
    "    w = w - learning_rate * grad_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2, figsize = (18,5))\n",
    "ax[0].plot(loss_record)\n",
    "ax[1].plot(accuracy_record)\n",
    "ax[0].set_xlabel('Epochs')\n",
    "ax[0].set_ylabel('Loss')\n",
    "ax[1].set_xlabel('Epochs')\n",
    "ax[1].set_ylabel('Accuracy');\n",
    "\n",
    "print(np.max(accuracy_record))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Now Testing on left out set\n",
    "\n",
    "**task** test the performance of your logistic regression on your left out test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# centre X\n",
    "X_test_centred = np.ones_like(X_test)\n",
    "X_test_centred[1:] = (X_test[1:] -X_test[1:].mean(axis=1,keepdims=True)) / (X_test[1:].std(axis=1,keepdims=True)+epsilon)\n",
    "\n",
    "Z_test = z(w,X_test_centred)\n",
    "F_test = f(Z_test)\n",
    "l = loss(y_test,F_test)\n",
    "\n",
    "print(l,accuracy(y_test,F_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The multi-layer perceptron (MLP)\n",
    "\n",
    "A MLP for binary classification, containing a single hidden layer can be written:\n",
    "\n",
    "$$ \\hat{y} = f_2 \\left( \\mathbf{W_2} f_1 \\left(\\mathbf{W_1}\\mathbf{X}\\right) \\right) $$\n",
    "\n",
    "where $f_2$ is a non-linear activation function for the hidden layer (we use ReLu), \n",
    "\n",
    "$$ \\text{Relu}(x) = \\text{max}(0,x)$$\n",
    "\n",
    "$f_1$ is  a non-linear activation function for the output layer (we use sigmoid for classification).  Note weight matrices t $\\mathbf{W_1}$ and $\\mathbf{W_2}$. \n",
    "\n",
    "In this toy example we will create a network with one hidden layer with 5 units. \n",
    "\n",
    "<img src=\"imgs/2layer.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "**Question** Given the shape of our input data, and the fact that we are still seeking the solution to a binary classification what are the number of input and output units for this problem (answer below)? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer here**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now go about implementing our simple network from scratch with gradient descent based optimisation\n",
    "\n",
    "### The forward pass\n",
    "\n",
    "Once again, we can write the forward pass as a staged computation:\n",
    "\n",
    "$$\n",
    "\\mathbf{Z}_1 = \\mathbf{W}_1^T \\mathbf{X} \\\\\n",
    "\\mathbf{F}_1 = \\text{max}(0,\\mathbf{Z_1}) \\\\\n",
    "\\mathbf{Z}_2 = \\mathbf{W}_2^T \\mathbf{F}_1 \\\\\n",
    "\\mathbf{F}_2 = \\dfrac{1}{1+e^{- \\mathbf{Z_2}}} \\\\\n",
    "\\mathbf{L}  =  - \\mathbf{Y} \\ln(\\mathbf{F_2}) - (1-\\mathbf{Y}) \\ln(1-\\mathbf{F_2})\n",
    "$$\n",
    "\n",
    "Let's implement the forward pass. \n",
    "\n",
    "### Task 3.1 - First write code for the ReLU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    # Answer\n",
    "    return x * (x>=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.2 Implement a forward pass of the MLP below: \n",
    "\n",
    "Remember $\\mathbf{W_1}$ and $\\mathbf{W_2}$ must now be initialised as with small random numbers. What dimension must  $\\mathbf{W_1}$ and $\\mathbf{W_2}$ be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### STUDENTS CODE HERE ####\n",
    "# Answer\n",
    "W1 = np.random.randn(5,X_train.shape[0])\n",
    "W2 = np.random.randn(1,5)\n",
    "Z1 = np.matmul(W1,X_centred)\n",
    "F1 = relu(Z1)\n",
    "Z2 = np.matmul(W2,F1)\n",
    "F2 = f(Z2) # recall f is the sigmoid function\n",
    "l = loss(y_train,F2) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The backwards pass\n",
    "\n",
    "The vectorised gradients of our MLP computation graph are, in reverse order, as follows:\n",
    "\n",
    "$$\\frac{\\delta L}{\\delta \\mathbf{F}_2}=\\frac{\\mathbf{F}_2-\\mathbf{Y}}{\\mathbf{F}_2(1-\\mathbf{F}_2)} \\\\\n",
    "\\frac{\\delta  \\mathbf{F}_2}{\\delta  \\mathbf{Z}_2}=\\mathbf{F}_2(1-\\mathbf{F}_2) \\\\\n",
    "\\frac{\\delta  \\mathbf{Z}_2}{\\delta  \\mathbf{W}_2}=\\mathbf{X} \\\\\n",
    "\\frac{\\delta  \\mathbf{Z}_2}{\\delta  \\mathbf{F}_1}=\\mathbf{W}^T_2\\\\\n",
    "\\frac{\\delta  \\mathbf{F}_1}{\\delta  \\mathbf{Z}_1}=1(\\mathbf{Z}_1 >0)\\\\\n",
    "\\frac{\\delta  \\mathbf{Z}_1}{\\delta  \\mathbf{W}_1}=\\mathbf{X}\\\\\n",
    "$$\n",
    "\n",
    "\n",
    "Combining these together using the chain rule we get (from lecture)\n",
    "\n",
    "<img src=\"MLPbackprop.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "\n",
    "**Task** implement the missing parts of the vectorised backward pass, and copy in the forward pass from above.\n",
    "\n",
    "**Hint** carefully consider the order in which the stages are combined (covered in the lecture). Check the dimensions of the outputs are as expected\n",
    "\n",
    "**Note** \n",
    "\n",
    "`dL_dZ1`=$$ \\frac{\\delta  L}{\\delta  \\mathbf{Z}_1}= \\frac{\\delta L}{\\delta \\mathbf{F}_2} \\frac{\\delta  \\mathbf{F}_2}{\\delta  \\mathbf{Z}_2}\\frac{\\delta  \\mathbf{Z}_2}{\\delta  \\mathbf{W}_2} \\frac{\\delta  \\mathbf{Z}_2}{\\delta  \\mathbf{F}_1} \\frac{\\delta  \\mathbf{F}_1}{\\delta  \\mathbf{Z}_1}    $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 1e-5\n",
    "\n",
    "# centre X\n",
    "X_centred = np.ones_like(X_train)\n",
    "X_centred[1:] = (X_train[1:] -X_train[1:].mean(axis=1,keepdims=True)) / (X_train[1:].std(axis=1,keepdims=True)+epsilon)\n",
    "\n",
    "# initialise w1, w2\n",
    "W1 = np.random.randn(5,X_train.shape[0])\n",
    "W2 = np.random.randn(1,5)\n",
    "\n",
    "# we'll store the loss and accuracy in these lists during training\n",
    "loss_record_mlp = []\n",
    "accuracy_record_mlp = []\n",
    "\n",
    "num_iterations = 2000\n",
    "learning_rate = 1e-2\n",
    "\n",
    "\n",
    "for i in range(num_iterations):\n",
    "    # forward pass - get predictions\n",
    "    \n",
    "    #### STUDENT CODE HERE####\n",
    "    # please keep the output names of each stage so they work later with backprop\n",
    "    Z1 = np.matmul(W1,X_centred)\n",
    "    F1 = relu(Z1)\n",
    "    Z2 = np.matmul(W2,F1)\n",
    "    F2 = f(Z2) # recall f is the sigmoid function\n",
    "    l = loss(y_train,F2) \n",
    "\n",
    "    # store the loss/ accuracy at this iteration\n",
    "    loss_record_mlp.append(l)\n",
    "    accuracy_record_mlp.append(accuracy(y_train,F2))\n",
    "\n",
    "    \n",
    "    #backwards pass to get gradients\n",
    "    dL_dW2=np.matmul(F2-y_train,F1.T) \n",
    "    dL_df2=np.matmul(W2.T,F2-y_train)  \n",
    "    df2_dZ1  = 1.0 *(Z1> 0)\n",
    "    \n",
    "    dL_dZ1=np.multiply(dL_df2,df2_dZ1)\n",
    "    dL_dW1 = np.matmul(dL_dZ1,X_centred.T)\n",
    "    dJ_dW2=(1/W2.shape[0])*dL_dW2 \n",
    "    dJ_dW1=(1/W1.shape[0])*dL_dW1 \n",
    "\n",
    "    # update the weights\n",
    "    W2 = W2 - learning_rate * dJ_dW2    \n",
    "    W1 = W1 - learning_rate * dJ_dW1\n",
    "    \n",
    "# plot loss and accuracy    \n",
    "fig, ax = plt.subplots(1,2, figsize = (18,5))\n",
    "ax[0].plot(loss_record_mlp)\n",
    "ax[1].plot(accuracy_record_mlp)\n",
    "ax[0].set_xlabel('Epochs')\n",
    "ax[0].set_ylabel('Loss')\n",
    "ax[1].set_xlabel('Epochs')\n",
    "ax[1].set_ylabel('Accuracy');\n",
    "\n",
    "print(accuracy(y_train,F2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the performance of the MLP\n",
    "\n",
    "**Task** test the performance of your logistic regression on your left out test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z1_test = np.matmul(W1,X_test_centred)\n",
    "F1_test = relu(Z1_test)\n",
    "Z2_test = np.matmul(W2,F1_test)\n",
    "F2_test = f(Z2_test) \n",
    "l = loss(y_test,F2_test) \n",
    "\n",
    "print(l,accuracy(y_test,F2_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework\n",
    "\n",
    "1. Using multiclass data from ??? implement a softmax multi-class classifier as \n",
    "    a) a single neuron\n",
    "    b) an MLP\n",
    "2. Try using a tanh or leaky relu in place of the relu function in the MLP classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
